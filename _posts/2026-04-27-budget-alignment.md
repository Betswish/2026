---
layout: distill
title: "Budget Alignment - Making Models Reason in the User's Language"
description: "We explore a two step multilingual alignment recipe for large language models to keep reasoning and answers in the user language while preserving accuracy."
date: 2026-04-27
future: true
htmlwidgets: true

authors:
  - name: Anonymous

bibliography: "2026-04-27-budget-alignment.bib"

toc:
  - name: "Introduction"
  - name: "What we try (Method in two steps)"
  - name: "Key contributions"
  - name: "RQ0 — Can small SFT reprogram a reasoning model's reasoning tone?"
  - name: "RQ1 — Does SFT help accuracy, or only language reasoning style?"
  - name: "RQ2 — When RL comes, how does GRPO help with accuracy?"
  - name: "RQ3 — Where should RL start from - Base or SFT?"
  - name: "RQ4 — Can we push the Pareto frontier instead of trading accuracy for language consistency?"
  - name: "RQ5 — Does model merging help?"
  - name: "Discussion - Where performance regresses, and potential solutions"
  - name: "Blog Summary - Practical takeaways"
  - name: "Limitations and threats to validity"
---
